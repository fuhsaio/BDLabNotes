# ç†µ Entropy
> è³‡æ–™é ˜åŸŸæŒ‡çš„æ˜¯ è³‡è¨Šç†µ  
> ç†µ æ˜¯éš¨æ©Ÿè®Šé‡ä¸ç¢ºå®šæ€§çš„åº¦é‡ï¼Œä¸ç¢ºå®šæ€§è¶Šå¤§ï¼Œç†µå€¼è¶Šå¤§  
> è‹¥éš¨æ©Ÿè®Šé‡é€€åŒ–æˆå®šå€¼ï¼Œç†µç‚º 0

[Entropy and bit é—œé€£](https://hackmd.io/@sXG2cRDpRbONCsrtz8jfqg/ry-0k0PwH) 

</br></br>

<img src="https://user-images.githubusercontent.com/86312099/123638361-ae1cb680-d851-11eb-8456-c635549bd32d.png" width="" height="">

</br>

a variable (event) : **X**   
with n possible values (outcomes) : **x1, x2 â€¦ xn**   
each outcome having probability : **p1, p2 â€¦ pn**    

> **Entropy ä»‹æ–¼ 0 ~ log<sub>2</sub>n**

</br></br>

# Mutual Information
### ğ¼(ğ‘‹,ğ‘Œ) = ğ»(ğ‘‹) + ğ»(ğ‘Œ) âˆ’ ğ»(ğ‘‹,ğ‘Œ)
> H(X,Y) is the join entropy of X and Y

</br>

## Example
<img src ="https://user-images.githubusercontent.com/86312099/123641245-a27ebf00-d854-11eb-87ea-87987ff94eea.png">





